{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning with Non-Ignorable Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Christine Hwang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ChrsitineHwang/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler as Standardize\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "from itertools import combinations\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "Missing data is defined to be non-ignorable if it is not missing at random. Non-ignorable missing data causes the inference based on observed data to lead to biased parameter estimates and can affect the significance of your results. This paper attempts to use probabilistic models to fill in missing data to reduce this bias\n",
    "\n",
    "### Example\n",
    "In the scenario where you have netflix movie ratings, you are more likely to rate movies that you really enjoy or really dislike, but rarely rate movies that you are neutral about. Therefore, most of the missing data tends to be from these middle ratings and the distribution of the observed data is shifted to the right from the true distribution of copmletely filled data. In this scenario, the probability of observing a particular response depends on the value itself. Therefore, ignoring missing data can lead to biased parameter estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GraphicalModel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This standard mixsture model shows a latent variable z behind each observation Y. In this example of a multinomial mixture model, $z_n$ represents the latent variable behind each entry which is pulled from a $\\theta_k$ prior. This means that there are k different latent variables that our multinomial distribution can come from. For each observed value given its latent variable, there is a $\\beta_{vmz}$ that represents the probability of any observation taking a specific value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just want to make sure I have the right interpretation of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the scenario where we represent the netflix movie ratings as a multinomial distribution. Then from the diagram above, $Y_{1n'}$ = the rating star count for movie 1 for person n' and we assume there are n # people. Then if $Y_{im}$ = v, that means that the star count for movie i from person m = v. To represent each $Y_m$ is a multinomial distribution, we will define it as Mult($\\theta_j$, M), where $\\theta_z$ = P(selecting word v from latent variable z) where M is the total number of ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z_n$ ~ Mult($\\pi$,1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi$ = [.3, .2, .1, .4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the probability of having a latent variable $z_1$ = .3, $z_2$ = .2, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we represent each $Y_{m}|z_i$ ~ Multi($\\theta_z$,N), we view $\\theta_z$ to be the probability distribution of ratings for a single movie. For example, $\\theta = [.1 .2 .7]$ means that the probability you give the movie 1 star is .1, probability you give it 2 stars is .2, and probability you give it three stars is .7, and you can set your M = N so that it follows a Multinomial($\\theta$, N). This is because N people give movie $m$ a rating and therefore the distribution of all the ratings that the N people could have given movie $m$ is Multinoimal given the $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the $\\beta$ estimates $\\theta$ is if we view $\\theta$ to be the probability distribution of ratings for a single movie. For example, $\\theta = [.1 .2 .7]$ means that the probability you give the movie 1 star is .1, probability you give it 2 stars is .2, and probability you give it three stars is .7, and you can set your M = N so that it follows a Multinomial($\\theta$, N). Then it would make sense that $Y_{m}$~Multi($\\beta$,N) which represents the ratings from N people for the $m^{th}$ movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPT-v Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Model2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extrapolate from the previous model to show the missing data. $\\mu_v$ = $P(R_m = 1 | Y_m = v)$, whic his the probability the value is missing given the true value of the data. In our movie rating example, this probability is higher is $Y_m = 2-3$ because we are less likely to see a rating for a mediocre movie. Need to clarify previous questions before analyzing this model further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope to apply this model of missing data to medical data. For example, in data analyzing blood pressure or cholesterol, we may see that the probability of seeing missing values for younger, healthier people is higher than for older people because they are less likely to have health complications that require these measurements. Doctors may assume that these measurements are not necessary for the checkup and therefore the data will not be missing at random. In order to use this probabilistic model to fill in missing values, I need to understanding how these categorical variables (if you just convert them to integers) can represent a multinomial distribution. If a person's blood pressure can take the values [10, 50, 100], and our $\\theta = [.2 .2 .6]$, does this mean that the person's blood pressure = Multinomial($\\theta$, 1)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"EM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\phi_{zn}$ = posterior distribution of the latent variable z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta_z$ = probability of that latent variable Z = z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\beta_{vmz}$ = probability that Y = v given latent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mu_v$ = probability that $Y_i$ is missing given is has value v."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$ and $\\lambda$ are intermediate variables without interpretable value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta = I(y_{mn} = v)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$u_v$ comes from a beta distribution because it is a prior for a Binomial Distribution\n",
    "\n",
    "$\\theta$ comes from a Dirichlet distribution\n",
    "\n",
    "$\\beta$ comes from a dirichlet distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Full Data\n",
    "I created the probability distribution of movies for there different movies. Movie 1 is a bad movie that is likely to get a lot of 1s. Beta 2 is just an average movie whose ratings will resemble a normal distribution centered at 3. Movie 3 is a great movie that is likely to get very high reviews. Concatentate these ratings to get your full data set of 100 movie ratings for these 3 movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "I don't think I'm coding this correctly so that the reviews are consistent with each person.. Walk through approach and ask if it makes intuitive sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simulate_data(z=1, n = 100, m = 3, v = 5):\n",
    "    \n",
    "    ### create the different betas\n",
    "    #########\n",
    "    # If there are three latent variables, then each movie will have 3 sets of probability distributions\n",
    "    # because there can be \"teen, young adult, old people\" categories that have different movie preferences.\n",
    "    #\n",
    "    # Input: z = 3, m = 3\n",
    "    # Output: [ (z1:m1 rating),(z1:m2 rating), (z1:m3 rating), (z2:m1 rating), etc]\n",
    "    #\n",
    "    #########\n",
    "    all_beta = []\n",
    "    for z_ in range(z):\n",
    "        beta = []\n",
    "        for m_ in range(m):\n",
    "            beta_temp = np.random.dirichlet(np.ones(v),size=1)[0]\n",
    "            beta.append(beta_temp)\n",
    "        all_beta.append(beta)\n",
    "        \n",
    "\n",
    "    # Generates which latent variable each user comes from    \n",
    "    z_list = np.random.randint(1,z+1,n)\n",
    "    complete_data = np.zeros((n,m))\n",
    "    for i in range(len(z_list)):\n",
    "        for m_ in range(m):\n",
    "            complete_data[i][m_] = np.argmax(np.random.multinomial(1,list(all_beta[z_list[i]-1][m_])))+1\n",
    "    return complete_data, all_beta,z_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# z=3\n",
    "# m = 5\n",
    "# v = 5\n",
    "# n = 1000\n",
    "# complete_data,true_beta,z_list = simulate_data(z = z,n=n,m=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mu Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper in study, we let $\\mu_v(s)$ = $s$($v$ − 3) + 0.5, where $s$ is the parameter that controls the strength of the effect. I will set $s$ = .075. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the example of a movie rating, It makes more sense that the movie will be missing reviews it is a neutral movie than a bad movie. Therefore, I reversed the probability so that it more resembles a bell shaped distribution where the probability of the rating being missing peaks when star = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.35 ,  0.425,  0.5  ,  0.425,  0.35 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range(v):\n",
    "#     mu[i] = -.075*abs(i-2)+.5\n",
    "# mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern for Missing Data (R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this, we want to create a matrix that indicates 1 if the data is observed and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull $\\theta, \\beta$ from Dirichlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $\\theta$ and $\\beta$ are good priors for categorical distribution and multinomial distributions, they are appropriate priors for $\\theta$ and $\\beta$ because both represent the probability of taking a specific categorical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### proxy for when it found convergence rather than iterating but very volatile results that don't really converge\n",
    "#### if i set epsilon to be smaller?\n",
    "\n",
    "def run_EM(theta,beta,phi,gamma,lambda_,v,m,z,n,complete_data,r):\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    ### store the past\n",
    "    past_beta = np.zeros((v,m,z))\n",
    "    past_gamma = np.zeros((m,z,n))\n",
    "    past_phi = np.zeros((z,n))\n",
    "    past_mu = np.zeros(v)\n",
    "    past_lambda_ = np.zeros((v,m,z,n))\n",
    "    past_theta = np.zeros(z)\n",
    "    \n",
    "    counter= 0\n",
    "    \n",
    "    eps = .01\n",
    "#     while(abs(past_beta-beta).sum() > eps and abs(past_gamma-gamma).sum() > eps and abs(past_mu-mu).sum() > eps and\n",
    "#          abs(past_phi-phi).sum() > eps and abs(past_lambda_-lambda_).sum() > eps and abs(past_theta-theta).sum() > eps): \n",
    "    for i in range(100):    \n",
    "\n",
    "        ### E step\n",
    "        past_phi = np.copy(phi)\n",
    "        past_lambda_ = np.copy(lambda_)\n",
    "        past_theta = np.copy(theta)\n",
    "        past_beta = np.copy(beta)\n",
    "        past_gamma = np.copy(gamma)\n",
    "        past_mu = np.copy(mu)\n",
    "        ###### lambda\n",
    "        for v_ in range(v):\n",
    "            for m_ in range(m):\n",
    "                for z_ in range(z):\n",
    "                    for n_ in range(n):\n",
    "                        lambda_[v_][m_][z_][n_] = ((complete_data[n_][m_]==(v_+1))*mu[v_]*beta[v_][m_][z_])**r[n_][m_]*((1-mu[v_])*beta[v_][m_][z_])**(1-r[n_][m_])\n",
    "\n",
    "\n",
    "        ###### gamma\n",
    "        gamma = lambda_.sum(axis = 0)\n",
    "\n",
    "        ##### vectorized phi\n",
    "        phi = np.tile(theta, (n,1)).T*gamma.prod(axis = 0)/(np.tile(theta, (n,1)).T*gamma.prod(axis = 0)).sum(axis=0)\n",
    "\n",
    "\n",
    "        ### M step\n",
    "        theta = phi.sum(axis=1)/phi.sum()\n",
    "\n",
    "        #vectorized beta\n",
    "        for v_ in range(v):\n",
    "            for m_ in range(m):\n",
    "                for z_ in range(z):\n",
    "                    sum_ = 0\n",
    "                    for n_ in range(n):\n",
    "                        sum_ += phi[z_][n_]*lambda_[v_][m_][z_][n_]/gamma[m_][z_][n_]\n",
    "                    beta[v_][m_][z_] = sum_\n",
    "\n",
    "        beta = beta/phi.sum(axis=1)\n",
    "\n",
    "\n",
    "        ######mu\n",
    "        for v_ in range(v):\n",
    "            num = 0\n",
    "            denom = 0\n",
    "            for n_ in range(n):\n",
    "                for z_ in range(z):\n",
    "                    for m_ in range(m):\n",
    "                        num += phi[z_][n_]*r[n_][m_]*lambda_[v_][m_][z_][n_]/gamma[m_][z_][n_]\n",
    "                        denom += phi[z_][n_]*lambda_[v_][m_][z_][n_]/gamma[m_][z_][n_]\n",
    "            mu[v_] = num/denom\n",
    "\n",
    "        if counter % 100 == 0:\n",
    "            print counter\n",
    "        counter += 1\n",
    "    end = time.time()\n",
    "    print(end - start)  \n",
    "    return mu,gamma,phi,beta,theta,lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_prior(v,m,z,n):\n",
    "    #mu\n",
    "    mu = np.ones(v)\n",
    "    for i in range(v):\n",
    "        mu[i] = -.075*abs(i-2)+.5\n",
    "    if z == 1:\n",
    "        theta = [1]\n",
    "    else:\n",
    "        c = ()\n",
    "        for i in range(z):\n",
    "            c = (1,)+c\n",
    "            \n",
    "        theta = np.random.dirichlet(c,1).reshape((z,))\n",
    "    beta = np.random.dirichlet((1,1,1,1,1),(z,m)).transpose()\n",
    "    \n",
    "    phi = np.ones((z,n))\n",
    "    gamma = np.ones((m,z,n))\n",
    "    lambda_ = np.ones((v,m,z,n))\n",
    "    \n",
    "    return mu,theta,beta,phi,gamma,lambda_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_missing(n,m,complete_data,mu):\n",
    "    ####### R matrix that decides which values are missing\n",
    "    r = np.ones((n,m))\n",
    "    for n_ in range(n):\n",
    "        for m_ in range(m):\n",
    "            val = complete_data[n_][m_]\n",
    "            if np.random.rand(1) < mu[val-1]:\n",
    "                r[n_][m_] = 0\n",
    "    return r\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def missing_value_filled_in(mu,phi,beta,n,m,missing_data):\n",
    "\n",
    "    ### fills in the data\n",
    "    for n_ in range(n):\n",
    "        for m_ in range(m):\n",
    "            latent = np.argmax(phi[:,n_])\n",
    "            if missing_data[n_][m_] == 0:\n",
    "                missing_data[n_][m_] = np.mean(np.argmax(np.random.multinomial(1000, beta[:,m_,latent])))+1\n",
    "    return missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z=1\n",
    "m = 5\n",
    "v = 5\n",
    "n = 10000\n",
    "complete_data,true_beta,z_list = simulate_data(z = z,n=n,m=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu,theta,beta,phi,gamma,lambda_ = create_prior(v,m,z,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ChrsitineHwang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:7: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "r = calculate_missing(n,m,complete_data,mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "373.909637928\n"
     ]
    }
   ],
   "source": [
    "mu,gamma,phi,beta,theta,lambda_ = run_EM(theta,beta,phi,gamma,lambda_,v,m,z,n,complete_data,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_data = r*complete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  5.,  4.,  2.,  4.],\n",
       "       [ 3.,  2.,  1.,  2.,  0.],\n",
       "       [ 1.,  2.,  4.,  4.,  0.],\n",
       "       ..., \n",
       "       [ 4.,  5.,  1.,  4.,  4.],\n",
       "       [ 0.,  5.,  5.,  4.,  5.],\n",
       "       [ 0.,  4.,  4.,  0.,  0.]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  5.,  4.,  2.,  4.],\n",
       "       [ 3.,  2.,  1.,  2.,  1.],\n",
       "       [ 1.,  2.,  4.,  4.,  1.],\n",
       "       ..., \n",
       "       [ 4.,  5.,  1.,  4.,  4.],\n",
       "       [ 1.,  5.,  5.,  4.,  5.],\n",
       "       [ 1.,  4.,  4.,  4.,  1.]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_filled_in = missing_value_filled_in(mu,phi,beta,n,m,np.copy(missing_data))\n",
    "model_filled_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.456366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.256218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.376665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.581685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.434114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.767226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.048175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.747987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.611629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.493243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.990485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.606908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.428094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.302509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.951134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.823141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.820500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.766966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.250399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.677074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.878525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.866992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.981698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.164410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.836258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.276656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.980745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.784534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.799016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.587654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.563541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.554579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.981970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.308096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.372149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.542201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.841271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.258140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.239477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.052719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.750042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.537988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.241073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.264484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.192672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.759374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.108222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.286005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13.810096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.480390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.219213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.779735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.431950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.765782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.317423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.569862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.267004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.583914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4          y\n",
       "0     0.0  5.0  4.0  2.0  4.0  11.456366\n",
       "1     3.0  2.0  1.0  2.0  0.0   8.256218\n",
       "2     1.0  2.0  4.0  4.0  0.0  10.376665\n",
       "3     4.0  2.0  0.0  4.0  1.0  14.581685\n",
       "4     0.0  1.0  4.0  0.0  1.0   8.434114\n",
       "5     0.0  4.0  0.0  0.0  0.0  14.767226\n",
       "6     0.0  2.0  1.0  4.0  1.0  11.048175\n",
       "7     0.0  0.0  4.0  0.0  0.0  15.747987\n",
       "8     3.0  0.0  2.0  4.0  0.0  12.611629\n",
       "9     0.0  1.0  1.0  4.0  2.0   7.493243\n",
       "10    0.0  0.0  2.0  4.0  5.0   9.990485\n",
       "11    3.0  0.0  0.0  4.0  0.0   8.606908\n",
       "12    1.0  0.0  2.0  4.0  1.0   7.428094\n",
       "13    0.0  0.0  5.0  0.0  1.0  12.302509\n",
       "14    1.0  0.0  0.0  0.0  2.0   7.951134\n",
       "15    4.0  2.0  0.0  0.0  0.0  11.823141\n",
       "16    1.0  0.0  0.0  4.0  4.0   9.820500\n",
       "17    0.0  3.0  1.0  4.0  0.0   8.766966\n",
       "18    1.0  2.0  2.0  0.0  5.0   2.250399\n",
       "19    4.0  1.0  0.0  1.0  0.0   5.677074\n",
       "20    1.0  1.0  0.0  2.0  1.0   4.878525\n",
       "21    1.0  0.0  0.0  2.0  1.0   6.866992\n",
       "22    1.0  5.0  0.0  2.0  1.0   7.981698\n",
       "23    1.0  0.0  0.0  0.0  0.0   8.164410\n",
       "24    0.0  1.0  0.0  0.0  1.0   8.836258\n",
       "25    1.0  5.0  5.0  4.0  1.0  13.276656\n",
       "26    4.0  0.0  2.0  4.0  2.0   9.980745\n",
       "27    0.0  2.0  0.0  0.0  0.0   9.784534\n",
       "28    0.0  4.0  0.0  4.0  0.0  13.799016\n",
       "29    1.0  0.0  0.0  4.0  1.0  11.587654\n",
       "...   ...  ...  ...  ...  ...        ...\n",
       "9970  1.0  0.0  4.0  0.0  1.0   7.563541\n",
       "9971  0.0  0.0  0.0  4.0  4.0  11.554579\n",
       "9972  0.0  3.0  4.0  2.0  0.0  12.981970\n",
       "9973  4.0  1.0  0.0  0.0  1.0  11.308096\n",
       "9974  1.0  0.0  3.0  2.0  0.0   8.372149\n",
       "9975  1.0  0.0  0.0  4.0  0.0  13.542201\n",
       "9976  0.0  2.0  4.0  0.0  4.0   7.841271\n",
       "9977  1.0  4.0  5.0  0.0  1.0  12.258140\n",
       "9978  0.0  0.0  0.0  2.0  1.0  14.239477\n",
       "9979  3.0  4.0  1.0  5.0  1.0  12.052719\n",
       "9980  1.0  5.0  0.0  0.0  0.0   9.750042\n",
       "9981  1.0  0.0  5.0  2.0  0.0   6.537988\n",
       "9982  1.0  5.0  1.0  4.0  1.0  10.241073\n",
       "9983  0.0  4.0  0.0  4.0  0.0  11.264484\n",
       "9984  1.0  4.0  5.0  4.0  2.0  12.192672\n",
       "9985  0.0  4.0  0.0  4.0  4.0   8.759374\n",
       "9986  3.0  0.0  0.0  4.0  0.0  14.108222\n",
       "9987  3.0  0.0  5.0  4.0  0.0  11.286005\n",
       "9988  5.0  5.0  4.0  2.0  4.0  13.810096\n",
       "9989  2.0  0.0  0.0  0.0  0.0   9.480390\n",
       "9990  2.0  2.0  2.0  0.0  0.0   9.219213\n",
       "9991  5.0  2.0  1.0  0.0  0.0  10.779735\n",
       "9992  0.0  4.0  4.0  0.0  1.0  13.431950\n",
       "9993  4.0  0.0  5.0  4.0  1.0  13.765782\n",
       "9994  0.0  0.0  0.0  4.0  1.0  11.317423\n",
       "9995  0.0  5.0  4.0  4.0  1.0  12.635400\n",
       "9996  1.0  0.0  0.0  4.0  1.0   8.159300\n",
       "9997  4.0  5.0  1.0  4.0  4.0   9.569862\n",
       "9998  0.0  5.0  5.0  4.0  5.0  11.267004\n",
       "9999  0.0  4.0  4.0  0.0  0.0  10.583914\n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data_y = pd.DataFrame(complete_data)\n",
    "missing_data = pd.DataFrame(missing_data)\n",
    "missing_data['y'] = complete_data_y[0]+complete_data_y[1] + complete_data_y[2]+complete_data_y[3]-complete_data_y[4]+np.random.normal(0,1,len(complete_data_y[0]))\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def standard_imputation(missing_data):\n",
    "    column_names = missing_data.columns\n",
    "    for col in column_names:\n",
    "        col_mean = np.mean(missing_data[col])\n",
    "        for i in range(len(missing_data[col])):\n",
    "            if missing_data[col][i] == 0:\n",
    "                missing_data[col][i] = col_mean\n",
    "    print missing_data.shape\n",
    "    return missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6)\n"
     ]
    }
   ],
   "source": [
    "standard_missing_data = standard_imputation(pd.DataFrame(np.copy(missing_data)))\n",
    "# standard_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_filled_in = pd.DataFrame(model_filled_in)\n",
    "model_filled_in['y'] = missing_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44464665438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ChrsitineHwang/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso_Reg(alpha = 0)\n",
    "lasso.fit(np.array(model_filled_in)[:,:-1],np.array(model_filled_in['y']))\n",
    "print lasso.score(np.array(model_filled_in)[:,:-1],np.array(model_filled_in['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.163717868339\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso_Reg(alpha = 1)\n",
    "lasso.fit(np.array(standard_missing_data)[:,:-1],np.array(standard_missing_data[5]))\n",
    "print lasso.score(np.array(standard_missing_data)[:,:-1],np.array(standard_missing_data[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_filled_in.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Markdown\n",
    "With the previous brute force method for phi, the time it took to run 10,000 iterations was 277.\n",
    "\n",
    "\n",
    "With the vectorized method for phi, the time it took to run 10,000 iterations was 256.\n",
    "\n",
    "\n",
    "After vectorizing the denominator of beta, it sped up to 235.\n",
    "\n",
    "Using the convergence proxy, I clearly don't need 10,000 iterations so maybe vectorizing not entirelly necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! My code actually works. Our original beta estimates were $[[.5,.2,.1,.1,.1],[.1,.2,.5,.2,.1],[.05,.1,.1,.35,.4]]$. We can see from the $\\beta$ estimates that for the first movie, you are more likely to give it a 1 and for the last movie, you are more likely to give it a 5 and for the second movie, you are most likely to give it a 3. That's what our initial beta estimates are!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept behind predicting is that now that we have a converged beta, for each of the missing values, we will pull the rating value $v$ from a multinomial distribution. How do I deal with this when there are latent variables? How do you know which latent variable each perosn is from? I can take the posterior distribution of the phi and take the argmax to identify which latent variable he or she comes from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fill_in_data = r*complete_data\n",
    "for n_ in range(n):\n",
    "    for m_ in range(m):\n",
    "        latent = np.argmax(phi[:,n_])\n",
    "        if fill_in_data[n_][m_] == 0:\n",
    "            fill_in_data[n_][m_] = np.argmax(np.random.multinomial(1, beta[:,m_,latent]))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  2.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_in_data[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_in_accuracy(complete,filled,rmse = True):\n",
    "    if not rmse:\n",
    "        difference = 0\n",
    "        total = 0\n",
    "        for n_ in range(n):\n",
    "            for m_ in range(m):\n",
    "                if (complete[n_][m_] + filled[n_][m_] <> 0):\n",
    "                    total += 1\n",
    "                    if complete[n_][m_] == filled[n_][m_]:\n",
    "                        difference += 1\n",
    "        print \"The total number of missing values is\", total\n",
    "        print \"The number of accurate filled values is\", difference\n",
    "    else:\n",
    "        mad = abs(complete_data[np.nonzero((1-r)*complete_data)]-fill_in_data[np.nonzero((1-r)*fill_in_data)]).mean()\n",
    "        print \"the mean absolute deviation is\",mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of missing values is 117\n",
      "The number of accurate filled values is 35\n",
      "the mean absolute deviation is 1.2905982906\n"
     ]
    }
   ],
   "source": [
    "fill_in_accuracy((1-r)*complete_data,(1-r)*fill_in_data, rmse=False)\n",
    "fill_in_accuracy((1-r)*complete_data,(1-r)*fill_in_data, rmse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to hard code with for loop for three of the predictors. Working on how to use matrix multiplication but can't see a clear pattern. Also not sure how to use $\\beta$ to actually predict the values after the fitting is done. Do I call np.random.multinomial($\\beta$, 1) then take the index of the array that has 1 success and fill in that missing value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "-- after talk with Weiwei on Tuesday, \n",
    "\n",
    "    -- create a function handle multiple latent variables. x\n",
    "    \n",
    "    -- speed up by vectorizing some of the code x\n",
    "    \n",
    "    -- run the loop using epsilon rather than just a lot of loops. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Links\n",
    "\n",
    "Description of Multinomial Mixture Models\n",
    "http://web.stanford.edu/~lmackey/stats306b/doc/stats306b-spring14-lecture3_scribed.pdf\n",
    "https://www.cs.princeton.edu/courses/archive/spring12/cos424/pdf/em-mixtures.pdf\n",
    "\n",
    "My specific Paper\n",
    "http://www.cs.ubc.ca/~bmarlin/research/presentations/lnimd_group_talk.pdf\n",
    "https://people.cs.umass.edu/~marlin/research/papers/aistat-lnimd.pdf\n",
    "\n",
    "Application of my paper\n",
    "http://ijcai.org/Proceedings/11/Papers/447.pdf\n",
    "http://www.cs.toronto.edu/~zemel/documents/cfmar-uai2007.pdf\n",
    "https://pdfs.semanticscholar.org/2845/eda7ce8de14e351d41182f92b73ece8873ef.pdf\n",
    "https://people.cs.umass.edu/~marlin/research/thesis/cfmlp.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
